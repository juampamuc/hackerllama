{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: \"01/20/2024\"\n",
    "twitter-card: true\n",
    "title: Sentence Embeddings. Cross-encoders and Re-ranking\n",
    "description: Deep Dive into Cross-encoders and Re-ranking\n",
    "image: embedding.png\n",
    "toc-depth: 5\n",
    "format:\n",
    "  html:\n",
    "    comments:\n",
    "      utterances:\n",
    "         repo: osanseviero/hackerllama\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings2/index.ipynb\" rel=\"nofollow\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This series aims to demystify embeddings and show you how to use them in your projects. The [first blog post](/blog/posts/sentence_embeddings) taught you how to use and scale up open-source embedding models, pick an existing model, current evaluation methods, and the state of the ecosystem. This second blog post will dive deeper into embeddings and explain the differences between bi-encoders and cross-encoders. Then, we'll dive into **retrieving and re-ranking**: we'll build a tool to answer questions about 400 AI papers. We'll briefly discuss about two different papers at the end. Enjoy!\n",
    "\n",
    "You can either read the content here or execute it in Google Colab by clicking the badge at the top of the page. Let’s dive into embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR\n",
    "\n",
    "Sentence Transformers supports two types of models: Bi-encoders and Cross-encoders. Bi-encoders are faster and more scalable, but cross-encoders are more accurate. Although both tackle similar high-level tasks, when to use one versus the other is quite different. Bi-encoders are better for search, and cross-encoders are better for classification and high-accuracy ranking. Let's dive into the details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "All the models we saw in the previous blog post were bi-encoders. Bi-encoders are models that encode the input text into a fixed-length vector. When you compute the similarity between two sentences, we usually encode the two sentences into two vectors and then compute the similarity between the two vectors (e.g., by using cosine similarity). We train bi-encoders to optimize the increase in the similarity between the query and relevant sentences and decrease the similarity between the query and the other sentences. This is why bi-encoders are better suited for search. As the previous blog post showed, bi-encoders are fast and easily scalable. If multiple sentences are provided, the bi-encoder will encode each sentence independently. This means that the sentence embeddings are independent of each other. This is a good thing for search, as we can encode millions of sentences in parallel. However, this also means that the bi-encoder doesn't know anything about the relationship between the sentences.\n",
    "\n",
    "When we use cross-encoders, we do something different. Cross-encoders encode the two sentences simultaneously and then output a classification score. The figure below shows the high-level differences\n",
    "\n",
    "![](cross_encoder.png)\n",
    "\n",
    "Why would you use one versus the other? Cross-encoders are slower and more memory intensive but also much more accurate. A cross-encoder is an excellent choice to compare a few dozen sentences. If you want to compare hundreds of thousands of sentences, a bi-encoder is a better choice, as otherwise a cross-encoder could take multiple hours. What if you care about accuracy and want to compare thousands of sentences efficiently? This is a typical case when you want to retrieve information. In those cases, an option is first to use a bi-encoder to reduce the number of candidates (i.e., get the top 20 most relevant examples) and then use a cross-encoder to get the final result. This is called re-ranking and is a common technique in information retrieval; we'll learn more about it later in this blog post!\n",
    "\n",
    "Given that the cross-encoder is more accurate, it's also a good option for tasks where subtle differences matter, such as medical or legal documents where a slight difference in wording can change the sentence's meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-encoders\n",
    "\n",
    "As mentioned, cross-encoders encode two texts simultaneously and then output a classification label. The cross-encoder first generates a single embedding that captures representations and their relationships. Compared to bi-encoder-generated embeddings (which are independent of each other), cross-encoder embeddings are dependent on each other. This is why cross-encoders are better suited for classification, and their quality is higher: they can capture the relationship between the two sentences! On the flip side, cross-encoders are slow if you need to compare thousands of sentences since they need to encode all the sentence pairs. \n",
    "\n",
    "Let's say you have four sentences, and you need to compare all the possible pairs:\n",
    "\n",
    "* A bi-encoder would need to encode each sentence independently, so it would need to encode four sentences. \n",
    "* A cross-encoder would need to encode all the possible pairs, so it would need to encode six sentences (AB, AC, AD, BC, BD, CD). \n",
    "\n",
    "Let's scale this. Let's say you have 100,000 sentences, and you need to compare all the possible pairs:\n",
    "\n",
    "* A bi-encoder would encode 100,000 sentences.\n",
    "* A cross-encoder would encode 4,999,950,000 pairs! (Using the [combinations formula](https://en.wikipedia.org/wiki/Binomial_coefficient): `n! / (r!(n-r)!)`, where n=100,000 and r=2). No wonder they don't scale well!\n",
    "\n",
    "Hence, it makes sense they are slower! \n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "Although cross-encoders have an intermediate embedding before the classification layer, it is not used for similarity search. This is because the cross-encoder is trained to optimize the classification loss, not the similarity loss. Hence, the embedding is specific to the classification task and not the similarity task.\n",
    "\n",
    ":::\n",
    "\n",
    "They can be used for different tasks. For example, for passage retrieval (given a question and a passage, is the passage relevant to the question?). Let's look at a quick code snippet with a small cross-encoder model trained for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install sentence_transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.152365 , -6.2870445], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\n",
    "scores = model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n",
    "                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another use case, more similar to what we did with bi-encoders, is to use cross-encoders for semantic similarity. For example, given two sentences, are they semantically similar? Although this is the same task we solved with bi-encoders, remember that cross-encoders are more accurate but slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46552283, 0.6350213 ], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CrossEncoder('cross-encoder/stsb-TinyBERT-L-4')\n",
    "scores = model.predict([(\"The weather today is beautiful\", \"It's raining!\"), \n",
    "                        (\"The weather today is beautiful\", \"Today is a sunny day\")])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and re-rank\n",
    "\n",
    "Now that we have learned about the differences between cross-encoders and bi-encoders, let's see how we can use them in practice by doing a two-stage retrieval and re-ranking system. This is a common technique in information retrieval, where you first retrieve the most relevant documents and then re-rank them using a more accurate model. This is a good option for comparing thousands of sentences efficiently and caring about accuracy. \n",
    "\n",
    "Suppose you have a corpus of 100,000 sentences and want to find the most relevant sentences to a given query. The first step is to use a bi-encoder to retrieve many candidates (to ensure recall). Then, you use a cross-encoder to re-rank the candidates and get the final result with high precision. This is a high-level overview of how the system would look like\n",
    "\n",
    "![](rerank.png)\n",
    "\n",
    "Let's try our luck by implementing a paper search system! We'll use a [AI Arxiv Dataset](https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked) in an excellent tutorial from [Pinecone](https://www.pinecone.io/learn/series/rag/rerankers/) about rerankers. The goal is to be able to ask AI questions and get relevant paper sections to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/osanseviero/.cache/huggingface/datasets/jamescalam___json/jamescalam--ai-arxiv-chunked-0d76bdc6812ffd50/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fbf9c02f2b4e6eb8cdf016446b66ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jamescalam/ai-arxiv-chunked\")\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the dataset, it's a chunked dataset of 400 Arxiv papers. Chunked means that sections are split into chunks/pieces of fewer tokens to make things more manageable for the model. Here is a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1910.01108',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       " 'id': '1910.01108',\n",
       " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
       " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
       " 'authors': ['Victor Sanh',\n",
       "  'Lysandre Debut',\n",
       "  'Julien Chaumond',\n",
       "  'Thomas Wolf'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20191002',\n",
       " 'updated': '20200301',\n",
       " 'references': [{'id': '1910.01108'}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get all the chunks, which we'll encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41584"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = dataset[\"train\"][\"chunk\"] \n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a bi-encoder to encode all the chunks into embeddings. We'll truncate long passages to 512 tokens. Note that short context is one of the downsides of many embedding models! We'll specifically use the [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) model, which is a small-sized model trained to encoder questions and passages into a similar embedding space. This model is a bi-encoder, so it's fast and scalable.\n",
    "\n",
    "Embedding all the 40,000+ passages takes around 30 seconds on my not-particularly special computer. Please note that we only need to generate the embeddings of the passages once, as we can save them to disk and load them later. In a production setting, you can save the embeddings to a database and load from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4a179b62044f97a4b7dcaf7c4c6d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "bi_encoder.max_seq_length = 256\n",
    "\n",
    "corpus_embeddings = bi_encoder.encode(chunks, convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now, let's provide a question and search for the relevant passage. To do this, we need to encode the question and then compute the similarity between the question and all the passages. Let's do this and look at the top hits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 14679, 'score': 0.6097552180290222},\n",
       " {'corpus_id': 17387, 'score': 0.5659530162811279},\n",
       " {'corpus_id': 39564, 'score': 0.5590510368347168},\n",
       " {'corpus_id': 14725, 'score': 0.5585878491401672},\n",
       " {'corpus_id': 5628, 'score': 0.5296251773834229},\n",
       " {'corpus_id': 14802, 'score': 0.5075011253356934},\n",
       " {'corpus_id': 9761, 'score': 0.49943411350250244},\n",
       " {'corpus_id': 14716, 'score': 0.4931946098804474},\n",
       " {'corpus_id': 9763, 'score': 0.49280521273612976},\n",
       " {'corpus_id': 20638, 'score': 0.4884325861930847},\n",
       " {'corpus_id': 20653, 'score': 0.4873950183391571},\n",
       " {'corpus_id': 9755, 'score': 0.48562008142471313},\n",
       " {'corpus_id': 14806, 'score': 0.4792214035987854},\n",
       " {'corpus_id': 14805, 'score': 0.475425660610199},\n",
       " {'corpus_id': 20652, 'score': 0.4740477204322815},\n",
       " {'corpus_id': 20711, 'score': 0.4703512489795685},\n",
       " {'corpus_id': 20632, 'score': 0.4695567488670349},\n",
       " {'corpus_id': 14750, 'score': 0.46810320019721985},\n",
       " {'corpus_id': 14749, 'score': 0.46809980273246765},\n",
       " {'corpus_id': 35209, 'score': 0.46695172786712646},\n",
       " {'corpus_id': 14671, 'score': 0.46657535433769226},\n",
       " {'corpus_id': 14821, 'score': 0.4637290835380554},\n",
       " {'corpus_id': 14751, 'score': 0.4585301876068115},\n",
       " {'corpus_id': 14815, 'score': 0.45775431394577026},\n",
       " {'corpus_id': 35250, 'score': 0.4569615125656128}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "query = \"what is rlhf?\"\n",
    "top_k = 25 # how many chunks to retrieve\n",
    "query_embedding = bi_encoder.encode(query, convert_to_tensor=True).cuda()\n",
    "\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 passage with score 0.6097552180290222 from http://arxiv.org/pdf/2204.05862:\n",
      "learning from human feedback, which we improve on a roughly weekly cadence. See Section 2.3.\n",
      "4This means that our helpfulness dataset goes ‘up’ in desirability during the conversation, while our harmlessness\n",
      "dataset goes ‘down’ in desirability. We chose the latter to thoroughly explore bad behavior, but it is likely not ideal\n",
      "for teaching good behavior. We believe this difference in our data distributions creates subtle problems for RLHF, and\n",
      "suggest that others who want to use RLHF to train safer models consider the analysis in Section 4.4.\n",
      "5\n",
      "1071081091010\n",
      "Number of Parameters0.20.30.40.50.6Mean Eval Acc\n",
      "Mean Zero-Shot Accuracy\n",
      "Plain Language Model\n",
      "RLHF\n",
      "1071081091010\n",
      "Number of Parameters0.20.30.40.50.60.7Mean Eval Acc\n",
      "Mean Few-Shot Accuracy\n",
      "Plain Language Model\n",
      "RLHFFigure 3 RLHF model performance on zero-shot and few-shot NLP tasks. For each model size, we plot\n",
      "the mean accuracy on MMMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, and\n",
      "TriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small\n",
      "\n",
      "\n",
      "Top 2 passage with score 0.5659530162811279 from http://arxiv.org/pdf/2302.07842:\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "\n",
      "\n",
      "Top 3 passage with score 0.5590510368347168 from http://arxiv.org/pdf/2307.09288:\n",
      "31\n",
      "5 Discussion\n",
      "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
      "limitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
      "models (Section 5.3).\n",
      "5.1 Learnings and Observations\n",
      "Our tuning process revealed several interesting results, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ’s abilities to temporally\n",
      "organize its knowledge, or to call APIs for external tools.\n",
      "SFT (Mix)\n",
      "SFT (Annotation)\n",
      "RLHF (V1)\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "Reward Model ScoreRLHF (V2)\n",
      "Figure 20: Distribution shift for progressive versions of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , from SFT models towards RLHF.\n",
      "Beyond Human Supervision. At the outset of the project, many among us expressed a preference for\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's store the IDs for later\n",
    "retrieval_corpus_ids = [hit['corpus_id'] for hit in hits]\n",
    "\n",
    "# Now let's print the top 3 results\n",
    "for i, hit in enumerate(hits[:3]):\n",
    "    sample = dataset[\"train\"][hit[\"corpus_id\"]]\n",
    "    print(f\"Top {i+1} passage with score {hit['score']} from {sample['source']}:\")\n",
    "    print(sample[\"chunk\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got the most similar chunks according to the high-recall but low-precision bi-encoder.\n",
    "\n",
    "Now, let's re-rank by using a higher-accuracy cross-encoder model. We'll use the [cross-encoder/ms-marco-MiniLM-L-6-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2) model. This model was trained with the MS MARCO Passage Retrieval dataset, a large dataset with real search questions and their relevant text passages. That makes the model quite suitable for making predictions using questions and passages. \n",
    "\n",
    "We'll use the same question and the top 10 chunks we got from the bi-encoder. Let's see the results! Recall that cross-encoders expect pairs, so we'll create pairs of the question and each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2227577 ,  5.048051  ,  1.2897239 ,  2.205767  ,  4.4136825 ,\n",
       "        1.2272772 ,  2.5638275 ,  0.81847703,  2.35553   ,  5.590804  ,\n",
       "        1.3877895 ,  2.9497519 ,  1.6762824 ,  0.7211323 ,  0.16303705,\n",
       "        1.3640019 ,  2.3106787 ,  1.5849439 ,  2.9696884 , -1.1079378 ,\n",
       "        0.7681126 ,  1.5945492 ,  2.2869687 ,  3.5448399 ,  2.056368  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import  CrossEncoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "cross_inp = [[query, chunks[hit['corpus_id']]] for hit in hits]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "cross_scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new value with the `cross-score` and sort by it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 20638, 'score': 0.4884325861930847, 'cross-score': 5.590804},\n",
       " {'corpus_id': 17387, 'score': 0.5659530162811279, 'cross-score': 5.048051},\n",
       " {'corpus_id': 5628, 'score': 0.5296251773834229, 'cross-score': 4.4136825},\n",
       " {'corpus_id': 14815, 'score': 0.45775431394577026, 'cross-score': 3.5448399},\n",
       " {'corpus_id': 14749, 'score': 0.46809980273246765, 'cross-score': 2.9696884},\n",
       " {'corpus_id': 9755, 'score': 0.48562008142471313, 'cross-score': 2.9497519},\n",
       " {'corpus_id': 9761, 'score': 0.49943411350250244, 'cross-score': 2.5638275},\n",
       " {'corpus_id': 9763, 'score': 0.49280521273612976, 'cross-score': 2.35553},\n",
       " {'corpus_id': 20632, 'score': 0.4695567488670349, 'cross-score': 2.3106787},\n",
       " {'corpus_id': 14751, 'score': 0.4585301876068115, 'cross-score': 2.2869687},\n",
       " {'corpus_id': 14725, 'score': 0.5585878491401672, 'cross-score': 2.205767},\n",
       " {'corpus_id': 35250, 'score': 0.4569615125656128, 'cross-score': 2.056368},\n",
       " {'corpus_id': 14806, 'score': 0.4792214035987854, 'cross-score': 1.6762824},\n",
       " {'corpus_id': 14821, 'score': 0.4637290835380554, 'cross-score': 1.5945492},\n",
       " {'corpus_id': 14750, 'score': 0.46810320019721985, 'cross-score': 1.5849439},\n",
       " {'corpus_id': 20653, 'score': 0.4873950183391571, 'cross-score': 1.3877895},\n",
       " {'corpus_id': 20711, 'score': 0.4703512489795685, 'cross-score': 1.3640019},\n",
       " {'corpus_id': 39564, 'score': 0.5590510368347168, 'cross-score': 1.2897239},\n",
       " {'corpus_id': 14802, 'score': 0.5075011253356934, 'cross-score': 1.2272772},\n",
       " {'corpus_id': 14679, 'score': 0.6097552180290222, 'cross-score': 1.2227577},\n",
       " {'corpus_id': 14716, 'score': 0.4931946098804474, 'cross-score': 0.81847703},\n",
       " {'corpus_id': 14671, 'score': 0.46657535433769226, 'cross-score': 0.7681126},\n",
       " {'corpus_id': 14805, 'score': 0.475425660610199, 'cross-score': 0.7211323},\n",
       " {'corpus_id': 20652, 'score': 0.4740477204322815, 'cross-score': 0.16303705},\n",
       " {'corpus_id': 35209, 'score': 0.46695172786712646, 'cross-score': -1.1079378}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]\n",
    "hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "msmarco_l6_corpus_ids = [hit['corpus_id'] for hit in hits] # save for later\n",
    "\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the cross-encoder does not agree as much with the bi-encoder. Surprisingly, some of the top cross-encoder results (14815 and 14749) have the lowest bi-encoder scores. This makes sense - bi-encoders compare the similitude of the question and the documents in the embedding space, while cross-encoders consider the relationship between the question and the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 passage with score 0.9668010473251343 from http://arxiv.org/pdf/2204.05862:\n",
      "Stackoverflow Good Answer vs. Bad Answer Loss Difference\n",
      "Python FT\n",
      "Python FT + RLHF(b)Difference in mean log-prob between good and bad\n",
      "answers to Stack Overﬂow questions.\n",
      "Figure 37 Analysis of RLHF on language modeling for good and bad Stack Overﬂow answers, over many\n",
      "model sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM\n",
      "ﬁnetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse\n",
      "at language modeling (left) .\n",
      "the RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than\n",
      "pure language modeling.\n",
      "B.8 Further Analysis of RLHF on Code-Model Snapshots\n",
      "As discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to\n",
      "elicit helpfulness, harmlessness, and honesty, which we refer to as ‘HHH’ prompts. In particular, they contain\n",
      "a couple of coding examples. Below is a description of what this prompt looks like:\n",
      "Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,\n",
      "\n",
      "\n",
      "Top 2 passage with score 0.9574587345123291 from http://arxiv.org/pdf/2302.07459:\n",
      "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
      "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
      "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
      "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
      "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
      "to control for the amount of RLHF training in the analysis of our experiments.\n",
      "3.2 Experiments\n",
      "3.2.1 Overview\n",
      "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
      "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
      "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
      "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
      "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
      "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
      "\n",
      "\n",
      "Top 3 passage with score 0.9408788084983826 from http://arxiv.org/pdf/2302.07842:\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, hit in enumerate(hits[:3]):\n",
    "    sample = dataset[\"train\"][hit[\"corpus_id\"]]\n",
    "    print(f\"Top {i+1} passage with score {hit['cross-score']} from {sample['source']}:\")\n",
    "    print(sample[\"chunk\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The results seem relevant to the query. What can we do to improve the results?\n",
    "\n",
    "Here we used [cross-encoder/ms-marco-MiniLM-L-6-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2), which is...well..it's three years old and it's tiny! It [was](https://www.sbert.net/docs/pretrained-models/ce-msmarco.html) one of the best re-ranking models some years ago.\n",
    "\n",
    "To pick a model, I suggest going to the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), clicking reranking, and selecting a good model that meets your requirements. The average column is a good proxy for general quality, but you might be particularly interested in a dataset (e.g., MSMarco in the retrieval tab).\n",
    "\n",
    "Note that some older models, such as MiniLM, are not there. Additionally, not all of these models are cross-encoders, so it's always important to experiment if adding the second-stage, slower re-ranker is worth it. Here are some that are interesting:\n",
    "\n",
    "1. [E5 Mistral 7B Instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct) (Dec 2023): This is a decoder-based embedder (not an encoder-based one as we learned before!). This means the model is massive for most applications (it has 7B params, which is two orders of magnitude higher than MiniLM!). This one is interesting because of the new trend of using encoder models rather than decoders, which could enable working with longer contexts. [Here](https://huggingface.co/papers/2401.00368) is the paper.\n",
    "2. [BAAI Reranker](https://huggingface.co/BAAI/bge-reranker-base) (Sep 2023): A high-quality re-ranking model with a decent size (278M parameters). Let's get the results with this and compare! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 passage with score 0.9668010473251343 from http://arxiv.org/pdf/2204.05862:\n",
      "Stackoverflow Good Answer vs. Bad Answer Loss Difference\n",
      "Python FT\n",
      "Python FT + RLHF(b)Difference in mean log-prob between good and bad\n",
      "answers to Stack Overﬂow questions.\n",
      "Figure 37 Analysis of RLHF on language modeling for good and bad Stack Overﬂow answers, over many\n",
      "model sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM\n",
      "ﬁnetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse\n",
      "at language modeling (left) .\n",
      "the RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than\n",
      "pure language modeling.\n",
      "B.8 Further Analysis of RLHF on Code-Model Snapshots\n",
      "As discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to\n",
      "elicit helpfulness, harmlessness, and honesty, which we refer to as ‘HHH’ prompts. In particular, they contain\n",
      "a couple of coding examples. Below is a description of what this prompt looks like:\n",
      "Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,\n",
      "\n",
      "\n",
      "Top 2 passage with score 0.9574587345123291 from http://arxiv.org/pdf/2302.07459:\n",
      "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
      "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
      "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
      "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
      "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
      "to control for the amount of RLHF training in the analysis of our experiments.\n",
      "3.2 Experiments\n",
      "3.2.1 Overview\n",
      "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
      "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
      "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
      "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
      "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
      "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
      "\n",
      "\n",
      "Top 3 passage with score 0.9408788084983826 from http://arxiv.org/pdf/2302.07842:\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same code as before, just different model\n",
    "cross_encoder = CrossEncoder('BAAI/bge-reranker-base')\n",
    "\n",
    "cross_inp = [[query, chunks[hit['corpus_id']]] for hit in hits]\n",
    "cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "for idx in range(len(cross_scores)):\n",
    "    hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "bge_corpus_ids = [hit['corpus_id'] for hit in hits]\n",
    "for i, hit in enumerate(hits[:3]):\n",
    "    sample = dataset[\"train\"][hit[\"corpus_id\"]]\n",
    "    print(f\"Top {i+1} passage with score {hit['cross-score']} from {sample['source']}:\")\n",
    "    print(sample[\"chunk\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the ranking of the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 passage. Bi-encoder 14679, Cross-encoder (MS Marco) 20638, BGE 14815\n",
      "Top 2 passage. Bi-encoder 17387, Cross-encoder (MS Marco) 17387, BGE 20638\n",
      "Top 3 passage. Bi-encoder 39564, Cross-encoder (MS Marco) 5628, BGE 17387\n",
      "Top 4 passage. Bi-encoder 14725, Cross-encoder (MS Marco) 14815, BGE 14679\n",
      "Top 5 passage. Bi-encoder 5628, Cross-encoder (MS Marco) 14749, BGE 9761\n",
      "Top 6 passage. Bi-encoder 14802, Cross-encoder (MS Marco) 9755, BGE 39564\n",
      "Top 7 passage. Bi-encoder 9761, Cross-encoder (MS Marco) 9761, BGE 20632\n",
      "Top 8 passage. Bi-encoder 14716, Cross-encoder (MS Marco) 9763, BGE 14725\n",
      "Top 9 passage. Bi-encoder 9763, Cross-encoder (MS Marco) 20632, BGE 9763\n",
      "Top 10 passage. Bi-encoder 20638, Cross-encoder (MS Marco) 14751, BGE 14750\n",
      "Top 11 passage. Bi-encoder 20653, Cross-encoder (MS Marco) 14725, BGE 14805\n",
      "Top 12 passage. Bi-encoder 9755, Cross-encoder (MS Marco) 35250, BGE 9755\n",
      "Top 13 passage. Bi-encoder 14806, Cross-encoder (MS Marco) 14806, BGE 14821\n",
      "Top 14 passage. Bi-encoder 14805, Cross-encoder (MS Marco) 14821, BGE 14802\n",
      "Top 15 passage. Bi-encoder 20652, Cross-encoder (MS Marco) 14750, BGE 14749\n",
      "Top 16 passage. Bi-encoder 20711, Cross-encoder (MS Marco) 20653, BGE 5628\n",
      "Top 17 passage. Bi-encoder 20632, Cross-encoder (MS Marco) 20711, BGE 14751\n",
      "Top 18 passage. Bi-encoder 14750, Cross-encoder (MS Marco) 39564, BGE 14716\n",
      "Top 19 passage. Bi-encoder 14749, Cross-encoder (MS Marco) 14802, BGE 14806\n",
      "Top 20 passage. Bi-encoder 35209, Cross-encoder (MS Marco) 14679, BGE 20711\n",
      "Top 21 passage. Bi-encoder 14671, Cross-encoder (MS Marco) 14716, BGE 20652\n",
      "Top 22 passage. Bi-encoder 14821, Cross-encoder (MS Marco) 14671, BGE 14671\n",
      "Top 23 passage. Bi-encoder 14751, Cross-encoder (MS Marco) 14805, BGE 20653\n",
      "Top 24 passage. Bi-encoder 14815, Cross-encoder (MS Marco) 20652, BGE 35209\n",
      "Top 25 passage. Bi-encoder 35250, Cross-encoder (MS Marco) 35209, BGE 35250\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    print(f\"Top {i+1} passage. Bi-encoder {retrieval_corpus_ids[i]}, Cross-encoder (MS Marco) {msmarco_l6_corpus_ids[i]}, BGE {bge_corpus_ids[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, we get very different results! Let's briefly look into some of them.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "I suggest doing something like `dataset[\"train\"][20638][\"chunk\"]` to print a particular result. Here is a quick summary of the results.\n",
    "\n",
    ":::\n",
    "\n",
    "The bi-encoder is good at getting some results related to RLHF, but it's struggling to get good, precise passages responding to what RLHF is. I looked at the top 5 results for each model. From looking at the passages, 17387 and 20638 are the only passages that really answer the question. Although the three models agree that 17387 is highly relevant, it's interesting that the bi-encoder ranks 20638 lowly, while the two cross-encoders rank it highly. You can find them here.\n",
    "\n",
    "\n",
    "\n",
    "| Corpus ID \t| Relevant text or summary                                                                 \t| Bi-encoder pos (from top 10) \t| MSMarco pos \t| BGE pos \t|\n",
    "|-----------\t|------------------------------------------------------------------------------------------\t|------------------------------\t|-------------\t|---------\t|\n",
    "| 14679     \t| Discusses implications and applications of RLHF but no definition.                       \t| 1                            \t| 20           \t| 4       \t|\n",
    "| 17387     \t| Describes the process of RLHF in detail and applications                                 \t| 2                            \t| 2           \t| 3       \t|\n",
    "| 39564     \t| This chunk is messy and is more of a discussion section intro than an answer             \t| 3                            \t| 18           \t| 6       \t|\n",
    "| 14725     \t|  Characteristics about RLHF but no definition of what it is                              \t| 4                            \t| 11           \t| 8       \t|\n",
    "| 20638     \t| \"increasingly popular technique for reducing harmful behaviors in large language models\" \t| 10                           \t| 1           \t| 2       \t|\n",
    "| 5628      \t| Discusses the reward modeling (a component) but does not define RLHF                     \t| 5                            \t| 3           \t| 16       \t|\n",
    "| 14815     \t| Discusses RLHF but does not define it                                                    \t| 24                            \t| 4           \t| 1       \t|\n",
    "| 14749     \t| Discusses impact of RLHF but it has no definition                                        \t| 19                            \t| 5           \t| 15       \t|\n",
    "| 9761      \t| Discusses the reward modeling (a component) but does not define RLHF                     \t| 7                            \t| 7           \t| 5       \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking is a frequent feature in libraries; `llamaindex` allows you to use a `VectorIndexRetriever` to retrieve and a `LLMRerank` to rerank (see [tutorial](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LLMReranker-Lyft-10k.html)), Cohere offers a [Rerank Endpoint](https://txt.cohere.com/rerank/) and [qdrant](https://qdrant.tech/articles/hybrid-search/) supports similar functionality. However, as you saw above, it's relatively simple to implement yourself. If you have a high-quality bi-encoder model, you can use it to rerank and benefit from its speed.\n",
    "\n",
    ":::{.callout-note title=\"LLMs as rerankers\"}\n",
    "\n",
    "Some people use a generative LLM as a reranker. For example, [OpenAI's Coobook](https://cookbook.openai.com/examples/search_reranking_with_cross-encoders) has an example in which they use GPT-3 as a reranker by building a prompt asking the model to determine if a document is relevant for the document. Although this shows the impressive capabilities of an LLM, it's usually not the best option for the task, as it will likely have worse quality, be more expensive, and be slower than a cross-encoder.\n",
    "\n",
    "Experiment and see what works best for your data. Using LLMs as rerankers can sometimes be helpful if your documents have very long contexts (for which bert-based models struggle).\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: SPECTER2\n",
    "\n",
    "If you're particularly excited about embeddings for scientific tasks, I suggest looking at [SPECTER2](https://huggingface.co/allenai/specter2_base) from AllenAI, a family of models that generate embeddings for scientific papers. These models can be used to do things such as predicting links, looking for nearest papers, find candidate papers for a given query, classify papers using the embeddings as features, and more!\n",
    "\n",
    "The base model was trained on [scirepeval](https://huggingface.co/datasets/allenai/scirepeval), a dataset of millions of triples of scientific paper citations. After being trained, the authors fine-tuned the model using [adapters](https://github.com/adapter-hub/adapters), a library for parameter-efficient fine-tuning (don't worry if you don't know what this is). The authors attached a small neural network, called an adapter, to the base model. This adapter is trained to perform a specific task, but training for a specific task requires much fewer data than training the whole model. Because of these differences, one needs to use `transformers` and `adapters` to run inference, e.g. by doing something like\n",
    "\n",
    "```\n",
    "model = AutoAdapterModel.from_pretrained('allenai/specter2_base')\n",
    "model.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"proximity\", set_active=True)\n",
    "```\n",
    "\n",
    "I recommend reading the model card to learn more about the model and its usage. You can also read the [paper](https://www.semanticscholar.org/paper/SPECTER%3A-Document-level-Representation-Learning-Cohan-Feldman/a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Augmented SBERT\n",
    "\n",
    "[Augmented SBERT](https://arxiv.org/abs/2010.08240) is a technique for collecting data to improve bi-encoders. Pre-training and fine-tuning bi-encoders require lots of data, so the authors suggested using cross-encoders to label a large set of input pairs and add that to the training data. For example, if you have very little labeled data, you can train a cross-encoder and then label unlabeled pairs, which can be used to train a bi-encoder.\n",
    "\n",
    "How do you generate the pairs? We can use random combinations of sentences and then label them using the cross-encoder. This would lead to mostly negative pairs and skew the label distribution. To avoid this, the authors explored different techniques:\n",
    "\n",
    "* With **Kernel Density Estimation (KDE)**, the goal is to have similar label distributions between a small, golden dataset and the augmentation dataset. This is achieved by dropping some negative pairs. Of course, this will be inefficient as you'll need to generate many pairs to get a few positive ones. \n",
    "* **BM25** is an algorithm used in search engines based on overlap (e.g., word frequency, length of document, etc.). Based on this, the authors get the top-k similar sentences to retrieve the k most similar sentences, and then, a cross-encoder is used to label them. This is efficient but will only be able to capture semantic similarity if there is little overlap between the sentences.\n",
    "* **Semantic Search Sampling**  trains a bi-encoder on the golden data and then used to sample other similar pairs. \n",
    "* **BM25 + Semantic Search Sampling** combines the two previous methods. This helps find lexical and semantically similar sentences.\n",
    "\n",
    "\n",
    "There are nice figures and example scripts to do this in the [Sentence Transformers docs](https://www.sbert.net/examples/training/data_augmentation/README.html).\n",
    "\n",
    "![Augmented SBERT - the image is from the original paper](augmented.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That was fun! We just learned to do one of the most common sentence embedding tasks: retrieve and rerank! We learned about the differences between bi-encoders and cross-encoders and when to use one versus the other. We also learned about some techniques to improve bi-encoders, such as augmented SBERT.\n",
    "\n",
    "Don't hesitate to change the code and play with it! If you like this blog post, don't hesitate to [leave a GitHub Star](https://github.com/osanseviero/hackerllama) or share it, that's always appreciated and motivating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Check\n",
    "\n",
    "1. What is the difference between bi-encoders and cross-encoders?\n",
    "2. Explain the different steps of reranking.\n",
    "3. How many embeddings would we need to generate to compare 30,000 sentences using a bi-encoder? How many times would we run inference with a cross-encoder?\n",
    "4. What are some techniques to improve bi-encoders?\n",
    "\n",
    "Now, you have solid foundations to implement your search system. As a follow-up, I suggest implementing a similar retrieve and rerank system with a different dataset. Explore how changing both retrieval and reranking models impact your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
